{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "304ef0a8-ac5c-4d15-a394-c16aeee9fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Global Settings \n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.inspection import permutation_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72f3b681-536f-4e0a-8eca-217c0f1ad098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Plot Settings\n",
    "\n",
    "avenir_font = FontProperties(family='Avenir')\n",
    "avenir_font_small = FontProperties(family='Avenir', size=6)\n",
    "\n",
    "# Apply Avenir globally for matplotlib\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Avenir',\n",
    "    'font.size': 10,\n",
    "    'axes.labelsize': 9,\n",
    "    'axes.titlesize': 10,\n",
    "    'xtick.labelsize': 8,\n",
    "    'ytick.labelsize': 8,\n",
    "    'legend.fontsize': 8,\n",
    "    'figure.dpi': 200\n",
    "})\n",
    "\n",
    "target_colors = {\n",
    "    'vitamin_d': '#1f77b4',        # blue\n",
    "    'hdl_cholesterol': '#ff7f0e',  # orange\n",
    "    'a1c': '#2ca02c'               # green\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84c177f6-61f2-4b25-ac93-f72dccb76184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data & metadata\n",
    "\n",
    "df = pd.read_feather(\"../data/processed/cleaned_nhanes.feather\")\n",
    "with open(\"../data/processed/metadata.json\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "cat_features = meta[\"cat_features\"]\n",
    "num_features = meta[\"num_features\"]\n",
    "target_cols = meta[\"target_cols\"]\n",
    "\n",
    "assert df[target_cols].notnull().all().all(), \"\\u274c Unexpected NaNs in target columns!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12f1a553-2097-4158-ab54-af86bde8e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload baseline models \n",
    "\n",
    "rf_model = joblib.load(\"../models/baseline_rf_model.pkl\")\n",
    "linear_model = joblib.load(\"../models/baseline_linear_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d65df13-aa7e-47ea-8729-43e398da71a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing and training data from 02 (note: y targets are scaled already. X are not bc they get scaled through the pipeline).\n",
    "\n",
    "X_train = pd.read_feather(\"../data/processed/X_train.feather\")\n",
    "X_test = pd.read_feather(\"../data/processed/X_test.feather\")\n",
    "y_train = pd.read_feather(\"../data/processed/y_train_scaled.feather\")\n",
    "y_test = pd.read_feather(\"../data/processed/y_test_scaled.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "026275c1-c051-4860-b487-9ad42f1b708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Pipeline \n",
    "\n",
    "# Numeric pipeline \n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline \n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine in column transformer \n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, num_features),\n",
    "    ('cat', categorical_pipeline, cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "787427f7-37d9-4644-a29f-a48e2ae02d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline models (already fit)\n",
    "baseline_models = {\n",
    "    \"Baseline: Random Forest\": rf_model,\n",
    "    \"Baseline: Linear Regression\": linear_model\n",
    "}\n",
    "\n",
    "# Tunable models\n",
    "auxiliary_models = {\n",
    "    \"Random Forest\": Pipeline([\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('regressor', MultiOutputRegressor(RandomForestRegressor(random_state=42)))\n",
    "    ]),\n",
    "    \"Gradient Boosting\": Pipeline([\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('regressor', MultiOutputRegressor(GradientBoostingRegressor(random_state=42)))\n",
    "    ]),\n",
    "    \"KNN\": Pipeline([\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('regressor', MultiOutputRegressor(KNeighborsRegressor()))\n",
    "    ]),\n",
    "    \"Ridge Regression\": Pipeline([\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('regressor', MultiOutputRegressor(Ridge()))\n",
    "    ]),\n",
    "    \"SVR\": Pipeline([\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('regressor', MultiOutputRegressor(SVR()))\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9cad658-0b7d-4c9c-8eaa-ae8fb875d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some params for the grid search\n",
    "\n",
    "# set n_splits for the CV #\n",
    "cv = KFold(n_splits=12, shuffle=True, random_state=42)\n",
    "\n",
    "# local hardware params\n",
    "n_cpus = 3\n",
    "#n_cpus = -1 # this is use all available cpus\n",
    "\n",
    "# for RandomizedSearchCV to assign number of random samples\n",
    "grid_samp = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40e9ec62-322e-45cc-9408-df84418fd0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 10 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewhollyday/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regressor Hyperparameter Tuning\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('regressor', MultiOutputRegressor(RandomForestRegressor(random_state=42)))\n",
    "])\n",
    "\n",
    "# LONG\n",
    "param_grid_rf = {\n",
    "    'regressor__estimator__n_estimators': [100, 300, 500, 800, 1000],\n",
    "    'regressor__estimator__max_depth': [None, 10, 20, 30, 50],\n",
    "    'regressor__estimator__min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'regressor__estimator__max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "# SHORT (for testing)\n",
    "# param_grid_rf = {\n",
    "#     'regressor__estimator__n_estimators': [100, 300],\n",
    "#     'regressor__estimator__max_depth': [None, 10, 30]\n",
    "# }\n",
    "\n",
    "# FULL GRID SEARCH\n",
    "# grid_rf = GridSearchCV(\n",
    "#     rf_pipeline, param_grid_rf, cv=cv, scoring='r2', n_jobs=n_cpus, verbose=1\n",
    "# )\n",
    "\n",
    "grid_rf = RandomizedSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_distributions=param_grid_rf,\n",
    "    n_iter=grid_samp,               \n",
    "    cv=cv,                 \n",
    "    scoring='r2',\n",
    "    n_jobs=n_cpus,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "grid_rf.fit(X_train, y_train)\n",
    "best_rf = grid_rf.best_estimator_\n",
    "best_rf_hp = grid_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16bdcbb2-1ee1-4c2a-ba64-6dfb3295f5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 10 candidates, totalling 120 fits\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Regressor Hyperparameter Tuning\n",
    "\n",
    "gbr_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('regressor', MultiOutputRegressor(GradientBoostingRegressor(random_state=42)))\n",
    "])\n",
    "\n",
    "# LONG\n",
    "param_grid_gbr = {\n",
    "    'regressor__estimator__n_estimators': [100, 300, 500, 800],\n",
    "    'regressor__estimator__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__estimator__max_depth': [3, 5, 7],\n",
    "    'regressor__estimator__subsample': [0.8, 1.0]\n",
    "}\n",
    "# SHORT (for testing)\n",
    "# param_grid_gbr = {\n",
    "#     'regressor__estimator__n_estimators': [100, 800],\n",
    "#     'regressor__estimator__learning_rate': [0.01, 0.1],\n",
    "# }\n",
    "\n",
    "# FULL GRID SEARCH\n",
    "# grid_gbr = GridSearchCV(\n",
    "#     gbr_pipeline, param_grid_gbr, cv=cv, scoring='r2', n_jobs=n_cpus, verbose=1\n",
    "# )\n",
    "\n",
    "grid_gbr = RandomizedSearchCV(\n",
    "    gbr_pipeline,\n",
    "    param_distributions=param_grid_gbr,\n",
    "    n_iter=grid_samp,           \n",
    "    cv=cv,                 \n",
    "    scoring='r2',\n",
    "    n_jobs=n_cpus,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "grid_gbr.fit(X_train, y_train)\n",
    "best_gbr = grid_gbr.best_estimator_\n",
    "best_gbr_hp = grid_gbr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2331387-e8ef-45f2-a52d-ac8879686839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 10 candidates, totalling 120 fits\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors Regressor Hyperparameter Tuning\n",
    "\n",
    "knn_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('regressor', MultiOutputRegressor(KNeighborsRegressor()))\n",
    "])\n",
    "\n",
    "# LONG\n",
    "param_grid_knn = {\n",
    "    'regressor__estimator__n_neighbors': [3, 5, 7, 9, 11, 15],\n",
    "    'regressor__estimator__weights': ['uniform', 'distance'],\n",
    "    'regressor__estimator__p': [1, 2]  # 1: Manhattan, 2: Euclidean\n",
    "}\n",
    "# SHORT (for testing)\n",
    "# param_grid_knn = {\n",
    "#     'regressor__estimator__n_neighbors': [3, 5, 9,],\n",
    "#     'regressor__estimator__weights': ['uniform', 'distance'],\n",
    "#     'regressor__estimator__p': [1]  # 1: Manhattan, 2: Euclidean\n",
    "# }\n",
    "\n",
    "# FULL GRID SEARCH\n",
    "# grid_knn = GridSearchCV(\n",
    "#     knn_pipeline, param_grid_knn, cv=cv, scoring='r2', n_jobs=n_cpus, verbose=1\n",
    "# )\n",
    "\n",
    "# Random selection from the grid to address runtime constraints\n",
    "grid_knn = RandomizedSearchCV(\n",
    "    knn_pipeline,\n",
    "    param_distributions=param_grid_knn,\n",
    "    n_iter=grid_samp,              \n",
    "    cv=cv,               \n",
    "    scoring='r2',\n",
    "    n_jobs=n_cpus,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "grid_knn.fit(X_train, y_train)\n",
    "best_knn = grid_knn.best_estimator_\n",
    "best_knn_hp = grid_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657a9e0c-6388-464a-821a-1ea44fdffb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 10 candidates, totalling 120 fits\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Regressor Hyperparameter Tuning\n",
    "\n",
    "svr_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('regressor', MultiOutputRegressor(SVR()))\n",
    "])\n",
    "\n",
    "# LONG\n",
    "param_grid_svr = {\n",
    "    'regressor__estimator__C': [0.1, 1.0, 10.0, 100.0],\n",
    "    'regressor__estimator__epsilon': [0.01, 0.1, 0.5],\n",
    "    'regressor__estimator__kernel': ['rbf', 'linear', 'poly']\n",
    "}\n",
    "# SHORT (for testing)\n",
    "# param_grid_svr = {\n",
    "#     'regressor__estimator__C': [1.0, 10.0],\n",
    "#     'regressor__estimator__epsilon': [0.01, 0.5],\n",
    "#     'regressor__estimator__kernel': ['rbf', 'linear',]\n",
    "# }\n",
    "\n",
    "# FULL GRID SEARCH\n",
    "# grid_svr = GridSearchCV(\n",
    "#     svr_pipeline, param_grid_svr, cv=cv, scoring='r2', n_jobs=n_cpus, verbose=1\n",
    "# )\n",
    "\n",
    "grid_svr = RandomizedSearchCV(\n",
    "    svr_pipeline,\n",
    "    param_distributions=param_grid_svr,\n",
    "    n_iter=grid_samp,               \n",
    "    cv=cv,                   \n",
    "    scoring='r2',\n",
    "    n_jobs=n_cpus,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "grid_svr.fit(X_train, y_train)\n",
    "best_svr = grid_svr.best_estimator_\n",
    "best_svr_hp = grid_svr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f706a747-91ab-42bd-889e-5f38a4079744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression Hyperparameter Tuning\n",
    "\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('regressor', MultiOutputRegressor(Ridge()))\n",
    "])\n",
    "\n",
    "# LONG\n",
    "param_grid_ridge = {\n",
    "    'regressor__estimator__alpha': [0.01, 0.1, 1.0, 10.0, 100.0]  \n",
    "}\n",
    "# SHORT (for testing)\n",
    "# param_grid_ridge = {\n",
    "#     'regressor__estimator__alpha': [0.1, 1.0, 10.0]  \n",
    "# }\n",
    "\n",
    "# FULL GRID SEARCH\n",
    "# grid_ridge = GridSearchCV(\n",
    "#     ridge_pipeline, param_grid_ridge, \n",
    "#     cv=cv, scoring='r2', n_jobs=n_cpus, verbose=1\n",
    "# )\n",
    "\n",
    "grid_ridge = RandomizedSearchCV(\n",
    "    ridge_pipeline,\n",
    "    param_distributions=param_grid_ridge,\n",
    "    n_iter=grid_samp,               \n",
    "    cv=cv,                   \n",
    "    scoring='r2',\n",
    "    n_jobs=n_cpus,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "grid_ridge.fit(X_train, y_train)\n",
    "best_ridge = grid_ridge.best_estimator_\n",
    "best_ridge_hp = grid_ridge.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8205a72-14dd-4472-96c7-fd22def3600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluate auxiliary models (default hyperparameters/ no grid search) ===\n",
    "\n",
    "auxiliary_results = []\n",
    "auxiliary_residuals = []\n",
    "\n",
    "for name, model in auxiliary_models.items():\n",
    "    display_name = f\"Default: {name}\"  # Rename model to reflect stage\n",
    "\n",
    "    print(f\"\\n🧠 Training {display_name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    for i, col in enumerate(target_cols):\n",
    "        true_vals = y_test[col].values\n",
    "        pred_vals = y_pred[:, i]\n",
    "\n",
    "        # Metrics\n",
    "        r2 = r2_score(true_vals, pred_vals)\n",
    "        rmse = mean_squared_error(true_vals, pred_vals, squared=False)\n",
    "\n",
    "        auxiliary_results.append({\n",
    "            \"Model\": display_name,\n",
    "            \"Target\": col,\n",
    "            \"R²\": r2,\n",
    "            \"RMSE\": rmse\n",
    "        })\n",
    "\n",
    "        # Residuals\n",
    "        auxiliary_residuals.append(pd.DataFrame({\n",
    "            \"Model\": display_name,\n",
    "            \"Target\": col,\n",
    "            \"True\": true_vals,\n",
    "            \"Predicted\": pred_vals,\n",
    "            \"Residual\": true_vals - pred_vals\n",
    "        }))\n",
    "\n",
    "# Format results\n",
    "auxiliary_df = pd.DataFrame(auxiliary_results)\n",
    "auxiliary_residuals_df = pd.concat(auxiliary_residuals, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0dd23-7aa8-4131-b205-c963619c6823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluate baseline models (default hyperparameters/ no grid search) === (Linear regression + RF)\n",
    "\n",
    "baseline_models = {\n",
    "    \"Baseline: Random Forest\": rf_model,\n",
    "    \"Baseline: Linear Regression\": linear_model\n",
    "}\n",
    "\n",
    "baseline_results = []\n",
    "baseline_residuals = []\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    for i, col in enumerate(target_cols):\n",
    "        true_vals = y_test[col].values\n",
    "        pred_vals = y_pred[:, i]\n",
    "\n",
    "        # Metrics\n",
    "        r2 = r2_score(true_vals, pred_vals)\n",
    "        rmse = mean_squared_error(true_vals, pred_vals, squared=False)\n",
    "\n",
    "        baseline_results.append({\n",
    "            \"Model\": name,\n",
    "            \"Target\": col,\n",
    "            \"R²\": r2,\n",
    "            \"RMSE\": rmse\n",
    "        })\n",
    "\n",
    "        # Residuals\n",
    "        baseline_residuals.append(pd.DataFrame({\n",
    "            \"Model\": name,\n",
    "            \"Target\": col,\n",
    "            \"True\": true_vals,\n",
    "            \"Predicted\": pred_vals,\n",
    "            \"Residual\": true_vals - pred_vals\n",
    "        }))\n",
    "\n",
    "# Format results\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "baseline_residuals_df = pd.concat(baseline_residuals, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9afaf0a-6a17-42f4-834f-432d4bbaf387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluate tuned models (default hyperparameters/ no grid search) === (Linear regression + RF)\n",
    "\n",
    "tuned_models = {\n",
    "    \"Tuned: Random Forest\": best_rf,\n",
    "    \"Tuned: Gradient Boosting\": best_gbr,\n",
    "    \"Tuned: KNN\": best_knn,\n",
    "    \"Tuned: Ridge\": best_ridge,\n",
    "    \"Tuned: SVR\": best_svr\n",
    "}\n",
    "\n",
    "# Collect metrics and residuals\n",
    "tuned_results = []\n",
    "tuned_residuals = []\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    for i, col in enumerate(target_cols):\n",
    "        true_vals = y_test[col].values\n",
    "        pred_vals = y_pred[:, i]\n",
    "\n",
    "        # Metrics\n",
    "        r2 = r2_score(true_vals, pred_vals)\n",
    "        rmse = mean_squared_error(true_vals, pred_vals, squared=False)\n",
    "\n",
    "        tuned_results.append({\n",
    "            \"Model\": name,\n",
    "            \"Target\": col,\n",
    "            \"R²\": r2,\n",
    "            \"RMSE\": rmse\n",
    "        })\n",
    "\n",
    "        # Residuals\n",
    "        tuned_residuals.append(pd.DataFrame({\n",
    "            \"Model\": name,\n",
    "            \"Target\": col,\n",
    "            \"True\": true_vals,\n",
    "            \"Predicted\": pred_vals,\n",
    "            \"Residual\": true_vals - pred_vals\n",
    "        }))\n",
    "        \n",
    "# Format results\n",
    "tuned_df = pd.DataFrame(tuned_results)\n",
    "tuned_residuals_df = pd.concat(tuned_residuals, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9007b-7ff9-4395-866f-d9b740100fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine metric and residual results\n",
    "combined_df = pd.concat([tuned_df, baseline_df, auxiliary_df], ignore_index=True)\n",
    "all_residuals_df = pd.concat([\n",
    "    baseline_residuals_df,\n",
    "    auxiliary_residuals_df,  # optional\n",
    "    tuned_residuals_df\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e2bcf-1ca8-4fed-b3fb-771c5791d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plot baseline models and tuned model metrics\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n",
    "\n",
    "for ax, metric in zip(axes, [\"R²\", \"RMSE\"]):\n",
    "    sns.barplot(data=combined_df, x=metric, y=\"Model\", hue=\"Target\", ax=ax, palette=target_colors)\n",
    "    ax.tick_params(axis='both', labelsize=8)\n",
    "    for tick in ax.get_yticklabels():\n",
    "        tick.set_fontproperties(avenir_font)\n",
    "if axes[1].get_legend():\n",
    "    axes[1].legend(\n",
    "        title=\"\",\n",
    "        bbox_to_anchor=(1.05, 1),\n",
    "        loc=\"upper left\",\n",
    "        frameon=False,\n",
    "        prop=avenir_font\n",
    "    )\n",
    "if axes[0].get_legend():\n",
    "    axes[0].get_legend().remove()\n",
    "    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8573d20d-01b0-40dd-9c9b-cd27ea9413b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "\n",
    "# === Setup\n",
    "models = all_residuals_df[\"Model\"].unique()\n",
    "targets = all_residuals_df[\"Target\"].unique()\n",
    "n_rows = len(models)\n",
    "n_cols = len(targets)\n",
    "\n",
    "# Plot settings\n",
    "bin_range = (-6, 6)\n",
    "bins = 40\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(3.5 * n_cols, 2.5 * n_rows), sharex=True, sharey=True)\n",
    "axes = axes.reshape(n_rows, n_cols)\n",
    "\n",
    "# === Plot\n",
    "for i, model in enumerate(models):\n",
    "    for j, target in enumerate(targets):\n",
    "        ax = axes[i, j]\n",
    "        subset = all_residuals_df[(all_residuals_df[\"Model\"] == model) & (all_residuals_df[\"Target\"] == target)]\n",
    "        \n",
    "        if not subset.empty:\n",
    "            sns.histplot(subset[\"Residual\"], bins=bins, kde=True, ax=ax, color=target_colors.get(target, \"gray\"))\n",
    "            ax.axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "        if i == 0:\n",
    "            ax.set_title(target, fontsize=9, fontproperties=avenir_font)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(model, fontsize=9, fontproperties=avenir_font)\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")\n",
    "\n",
    "        ax.set_xlim(bin_range)\n",
    "        \n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.tick_params(axis='both', labelsize=7)\n",
    "        for tick in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "            tick.set_fontproperties(avenir_font_small)\n",
    "\n",
    "plt.suptitle(\"Residual Distributions by Model and Target\", fontproperties=avenir_font, fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f68d7-25fe-4b56-9b00-b882b1e0aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics across many CVs\n",
    "\n",
    "cv_folds = [3, 6, 12, 24, 48]\n",
    "\n",
    "tuned_CVopt_results = []\n",
    "\n",
    "for model_name, base_model in tuned_models.items():\n",
    "    for k in cv_folds:\n",
    "        print(f\"=== {model_name} | CV Folds: {k} ===\")\n",
    "        \n",
    "        cv = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        model = clone(base_model)\n",
    "\n",
    "        # R² per target\n",
    "        r2_scores = cross_val_score(\n",
    "            model, X_train, y_train,\n",
    "            cv=cv, scoring='r2', n_jobs=n_cpus\n",
    "        )\n",
    "\n",
    "        # RMSE per target (via negative MSE)\n",
    "        neg_mse_scores = cross_val_score(\n",
    "            model, X_train, y_train,\n",
    "            cv=cv,\n",
    "            scoring=make_scorer(mean_squared_error, greater_is_better=False),\n",
    "            n_jobs=n_cpus\n",
    "        )\n",
    "        rmse_scores = np.sqrt(-neg_mse_scores)\n",
    "\n",
    "        # Save average metrics for each target\n",
    "        for i, col in enumerate(y_train.columns):\n",
    "            tuned_CVopt_results.append({\n",
    "                \"Model\": f\"{model_name} (CV={k})\",\n",
    "                \"Target\": col,\n",
    "                \"CV Folds\": k,\n",
    "                \"R²\": r2_scores[i],\n",
    "                \"RMSE\": rmse_scores[i]\n",
    "            })\n",
    "\n",
    "# Combine into a DataFrame\n",
    "tuned_CVopt_df = pd.DataFrame(tuned_CVopt_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1094c-0140-451b-9e45-ea2212a0de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_CVopt_df[\"Model Type\"] = tuned_CVopt_df[\"Model\"].str.extract(r\"^(Tuned: .+?) \\(CV=\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True)\n",
    "\n",
    "# R² Plot\n",
    "sns.lineplot(\n",
    "    data=tuned_CVopt_df,\n",
    "    x=\"CV Folds\", y=\"R²\",\n",
    "    hue=\"Model Type\",\n",
    "    marker=\"o\", ax=axes[0],\n",
    "    errorbar=None\n",
    ")\n",
    "axes[0].set_title(\"\", fontproperties=avenir_font)\n",
    "axes[0].tick_params(axis='both', labelsize=8)\n",
    "axes[0].get_legend().remove()  # remove left legend\n",
    "\n",
    "# RMSE Plot\n",
    "sns.lineplot(\n",
    "    data=tuned_CVopt_df,\n",
    "    x=\"CV Folds\", y=\"RMSE\",\n",
    "    hue=\"Model Type\",\n",
    "    marker=\"o\", ax=axes[1],\n",
    "    errorbar=None\n",
    ")\n",
    "axes[1].set_title(\"\", fontproperties=avenir_font)\n",
    "axes[1].tick_params(axis='both', labelsize=8)\n",
    "axes[1].legend(\n",
    "    title=\"Model Type\",\n",
    "    bbox_to_anchor=(1.05, 1),\n",
    "    loc=\"upper left\",\n",
    "    frameon=False,\n",
    "    prop=avenir_font\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429ecec-b805-427c-9f00-994e3ae0517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Dictionary of best trained pipelines\n",
    "best_models = {\n",
    "    \"Random Forest\": best_rf,\n",
    "    \"Gradient Boosting\": best_gbr,\n",
    "    \"KNN\": best_knn,\n",
    "    \"Ridge\": best_ridge,\n",
    "    \"SVR\": best_svr\n",
    "}\n",
    "\n",
    "X_val = X_train.copy()\n",
    "y_val = y_train.copy()\n",
    "\n",
    "importances_all = []\n",
    "\n",
    "for model_name, pipe in best_models.items():\n",
    "    print(f\"\\n→ Extracting feature importances for {model_name}\")\n",
    "\n",
    "    # Get preprocessor and transform features\n",
    "    preprocessor = pipe.named_steps[\"preprocessing\"]\n",
    "    X_transformed = preprocessor.transform(X_val)\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "    # Get regressor and unwrap estimators\n",
    "    regressor = pipe.named_steps[\"regressor\"]\n",
    "    if isinstance(regressor, MultiOutputRegressor):\n",
    "        estimators = regressor.estimators_\n",
    "    else:\n",
    "        estimators = [regressor]\n",
    "\n",
    "    for i, col in enumerate(y_val.columns):\n",
    "        print(f\"   Target: {col}\")\n",
    "        model = estimators[i]\n",
    "\n",
    "        # Tree-based models\n",
    "        if hasattr(model, \"feature_importances_\"):\n",
    "            importances = model.feature_importances_\n",
    "\n",
    "        # Ridge\n",
    "        elif hasattr(model, \"coef_\"):\n",
    "            importances = np.abs(model.coef_)\n",
    "\n",
    "        # Permutation-based (KNN, SVR)\n",
    "        else:\n",
    "            result = permutation_importance(\n",
    "                model, X_transformed, y_val[col],\n",
    "                n_repeats=5, random_state=42, n_jobs=n_cpus\n",
    "            )\n",
    "            importances = result.importances_mean\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"Model\": model_name,\n",
    "            \"Target\": col,\n",
    "            \"Feature\": feature_names,\n",
    "            \"Importance\": importances\n",
    "        }).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "        importances_all.append(df)\n",
    "\n",
    "# Combine all\n",
    "all_importances_df = pd.concat(importances_all, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030ea462-3392-475e-97b5-a804d7c9ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_importances_df[\"NormWithinTarget\"] = (\n",
    "    all_importances_df\n",
    "    .groupby([\"Model\", \"Target\"])[\"Importance\"]\n",
    "    .transform(lambda x: x / x.sum() if x.sum() > 0 else 0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a79719e-e5b9-4ef5-87ea-e835465db738",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 20\n",
    "\n",
    "top_features_df = (\n",
    "    all_importances_df\n",
    "    .groupby([\"Model\", \"Target\"], group_keys=False)\n",
    "    .apply(lambda df: df.nlargest(top_n, \"NormWithinTarget\"))\n",
    ")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=top_features_df,\n",
    "    kind=\"bar\",\n",
    "    x=\"NormWithinTarget\", y=\"Feature\",\n",
    "    hue=\"Target\",\n",
    "    col=\"Model\", row=\"Target\",\n",
    "    palette=target_colors,\n",
    "    sharex=True, sharey=False,\n",
    "    height=4, aspect=1.4\n",
    ")\n",
    "\n",
    "g.set_titles(\"{col_name} – {row_name}\")\n",
    "g.set_axis_labels(\"Percent Contribution (within target)\", \"\")\n",
    "if g._legend:\n",
    "    g._legend.set_title(\"\")\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_xlim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18908e8d-56fe-48c0-afa4-1f46042f350b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d85f49-e925-49ba-aba6-d42c55cceb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8f4c10-f912-43b5-a420-8c8d46983be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c9c7ee-dca2-441b-a9da-979c323c5e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26404d-31b1-4be3-95e5-bbec210db69b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c9309-bdf9-40af-9dd9-730b96f5b496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54719de-6795-4339-a881-83d849a392e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7fe08-afb6-47ae-b65c-a7c67959d190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
